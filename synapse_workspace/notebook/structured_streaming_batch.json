{
	"name": "structured_streaming_batch",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "TinyPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a85163a5-0802-4df0-a2ad-b276b7e036e0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/64e274c3-3493-4116-90f8-9c3098aaa268/resourceGroups/rg-kboks-test1/providers/Microsoft.Synapse/workspaces/azsynapsewksn6rjrc/bigDataPools/TinyPool",
				"name": "TinyPool",
				"type": "Spark",
				"endpoint": "https://azsynapsewksn6rjrc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TinyPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import col, to_timestamp, from_json, lit\n",
					"from pyspark.sql.types import *"
				],
				"execution_count": 86
			},
			{
				"cell_type": "code",
				"source": [
					"def readStream_EventHub(spark, source):\n",
					"    source_connectionString = source[\"connectionString\"]\n",
					"\n",
					"    ehConf = {'eventhubs.connectionString': spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(source_connectionString)}\n",
					"\n",
					"    return spark.readStream.format(\"eventhubs\").options(**ehConf).load().select(\n",
					"        col(\"body\").cast(\"string\").alias(\"body\"),\n",
					"        to_timestamp(col(\"enqueuedTime\")).alias(\"message_enqueued_time\")\n",
					"    )\n",
					"\n",
					"def readStream_parquet(spark, source):\n",
					"\n",
					"    spark.conf.set(\"spark.sql.streaming.schemaInference\", 'true')\n",
					"    return spark.readStream.parquet(**source).withColumn('file_path', input_file_name())\n",
					"\n",
					"def get_source(spark, source_type, source):\n",
					"\n",
					"    print(f'Reading source {source_type}')\n",
					"    if source_type == 'EventHub':\n",
					"        df = readStream_EventHub(spark, source)\n",
					"    elif source_type == 'Parquet':\n",
					"        df = readStream_parquet(spark, source)\n",
					"    return df"
				],
				"execution_count": 93
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def stream_process(\n",
					"    spark, source_type, source,\n",
					"    bronze_path, silver_path, gold_path, checkpoint_path,\n",
					"    bronze_transformations, silver_transformations, gold_transformations, write_mode=\"append\"\n",
					"):\n",
					"    print('Process started')\n",
					"\n",
					"    df_source = get_source(spark, source_type, source)\n",
					"\n",
					"\n",
					"    def write2table(df_source, epoch_id):\n",
					"        batch_id = int(epoch_id)\n",
					"\n",
					"        df_bronze = bronze_transformations(df_source)\n",
					"        print(f'Number of bronze records: {df_bronze.count()}')\n",
					"    \n",
					"        df_silver = silver_transformations(df_bronze)\n",
					"        print(f'Number of silver records: {df_silver.count()}')\n",
					"\n",
					"        df_gold = gold_transformations(df_silver)\n",
					"        print(f'Number of gold records: {df_gold.count()}')\n",
					"\n",
					"        print('Writing bronze, silver and gold')\n",
					"        df_bronze.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").save(bronze_path)\n",
					"        df_silver.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").save(silver_path)\n",
					"        df_gold.write.mode(write_mode).format(\"delta\").option(\"mergeSchema\", \"true\").save(gold_path)\n",
					"\n",
					"    query = df_source.writeStream.outputMode(\"update\").queryName('BigDataSummitQuery')\n",
					"\n",
					"    query = query.option(\"checkpointLocation\", checkpoint_path).foreachBatch(write2table).start()\n",
					""
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# parameters\n",
					"source_type = 'EventHub'\n",
					"source = {\"connectionString\": 'Endpoint=sb://azeventhubnsn6rjrc.servicebus.windows.net/;SharedAccessKeyName=event-hub-policy-kb;SharedAccessKey=VpejHpULoxv8N9nnrUpTaAIXEIaXnKNGI+AEhAv9wSQ=;EntityPath=azeventhubn6rjrc'}\n",
					"write_mode = 'append'\n",
					"bronze_path = 'abfss://bronze@azrawdatalaken6rjrc.dfs.core.windows.net/delta/stream_analytics_delta/'\n",
					"silver_path = 'abfss://silver@azcurateddatalaken6rjrc.dfs.core.windows.net/delta/stream_analytics_delta/'\n",
					"gold_path = 'abfss://gold@azcurateddatalaken6rjrc.dfs.core.windows.net/delta/stream_analytics_delta/'\n",
					"checkpoint_path = 'abfss://bronze@azrawdatalaken6rjrc.dfs.core.windows.net/delta/stream_analytics_delta/checkpoints/'\n",
					"\n",
					"\n",
					"def bronze_transformations(df):\n",
					"    df = df.withColumn(\"bronze\", col(\"json\"))\n",
					"    return df\n",
					"\n",
					"def silver_transformations(df):\n",
					"    df = df.withColumn(\"silver\", col(\"json\"))\n",
					"    return df\n",
					"\n",
					"def gold_transformations(df):\n",
					"    df = df.withColumn(\"gold\", col(\"json\"))\n",
					"    return df\n",
					"\n",
					""
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"stream_process(\n",
					"    spark, source_type, source,\n",
					"    bronze_path, silver_path, gold_path, checkpoint_path,\n",
					"    bronze_transformations, silver_transformations, gold_transformations, write_mode)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print('Process started')\n",
					"\n",
					"df_source = get_source(spark, source_type, source)"
				],
				"execution_count": 95
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def bronze_transformations(df):\n",
					"    df = df.withColumn(\"extra_column\", col(\"message_enqueued_time\"))\n",
					"    return df\n",
					"\n",
					"def write2table(df_source, epoch_id):\n",
					"    batch_id = int(epoch_id)\n",
					"\n",
					"    df_bronze = bronze_transformations(df_source)\n",
					"\n",
					"    print(f'Number of bronze records: {df_bronze.count()}')\n",
					"    print('Writing bronze')\n",
					"    df_bronze.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").save(bronze_path)"
				],
				"execution_count": 114
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Update mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.\n",
					"query = df_source.writeStream.outputMode(\"update\").queryName('test').trigger(processingTime='2 seconds').option(\"checkpointLocation\", checkpoint_path).foreachBatch(write2table).start()\n",
					"query.awaitTermination()\n",
					"\n",
					""
				],
				"execution_count": 115
			}
		]
	}
}